tune_parameter(data[1:1000, ], label[1:1000], data[1001:2000, ], label[1001:2000], type = "linear", C = 1)
devtools::load_all()
tune_parameter(data[1:1000, ], label[1:1000], data[1001:2000, ], label[1001:2000], type = "linear", C = 1)
knitr::opts_chunk$set(collapse = TRUE, comment = "#>", cache = FALSE,
autodep = FALSE, cache.comments = FALSE,
fig.width = 8, fig.height = 5)
suppressPackageStartupMessages(library(BinarySVMXHT))
library(BinarySVMXHT)
dataset = read.csv("StudentDepressionDataset.csv")
names(dataset)
head(dataset, 6)
plot_hist(dataset, "Gender")
plot_hist(dataset, "Age")
plot_hist(dataset, "City")
dataset = dataset[, -which(names(dataset) %in% c("id", "Job.Satisfaction", "Work.Pressure"))]
# Create a list which maps the character feature columns to numeric.
mapping = list(Gender = c("Female", "Male"),
Sleep.Duration = c("Less than 5 hours", "5-6 hours", "7-8 hours", "More than 8 hours", "Others"),
Dietary.Habits = c("Unhealthy", "Moderate", "Healthy"),
Have.you.ever.had.suicidal.thoughts.. = c("No", "Yes"),
Family.History.of.Mental.Illness = c("No", "Yes"))
dataset = preprocess(dataset, mapping, "Depression")
head(dataset, 6)
data = dataset[, 1:11]
label = dataset[, 12]
linear.model = svm(data[1:100, ], label[1:100])
linear.model
linear.model.partial = svm(data, label, n = 100, features = names(data)[1:8])
linear.model.partial
select_features(data, label, n =  8, show = TRUE)
select_features(data, label, min = 0.2)
predict(linear.model, data[101:200, ])
test_accuracy(linear.model, data[101:200, ], label[101:200])
predict(linear.model.partial, data[101:200, ])
test_accuracy(linear.model.partial, data[101:200, ], label[101:200])
tune_parameter(data[1:1000, ], label[1:1000], data[1001:2000, ], label[1001:2000], type = "linear", C = 1)
tune_parameter(data[1:200, ], label[1:200], data[201:400, ], label[201:400], type = "polynomial", C = c(1, 10, 100), degree = c(2, 3, 4))
tune_parameter(data[1:200, ], label[1:200], data[201:400, ], label[201:400], type = "rbf", C = c(1, 10, 100), gamma = c(0.01, 0.1, 1))
tune_parameter(data[1:150, ], label[1:150], data[151:300, ], label[150:300], type = "linear", C = c(1, 10, 100))
tune_parameter(data[1:150, ], label[1:150], data[151:300, ], label[151:300], type = "linear", C = c(1, 10, 100))
tune_parameter(data[1:300, ], label[1:300], data[301:600, ], label[301:600], type = "polynomial", C = c(1, 10, 100), degree = c(2, 3))
tune_parameter(data[1:300, ], label[1:300], data[301:600, ], label[301:600], type = "rbf", C = c(1, 10, 100), gamma = c(0.05, 0.1, 0.2))
tune_parameter(data[1:300, ], label[1:300], data[301:600, ], label[301:600], type = "rbf", C = c(1, 10, 100), gamma = c(0.05, 0.1, 0.2))
knitr::opts_chunk$set(collapse = TRUE, comment = "#>", cache = FALSE,
autodep = FALSE, cache.comments = FALSE,
fig.width = 8, fig.height = 5)
suppressPackageStartupMessages(library(BinarySVMXHT))
devtools::load_all()
knitr::opts_chunk$set(collapse = TRUE, comment = "#>", cache = FALSE,
autodep = FALSE, cache.comments = FALSE,
fig.width = 8, fig.height = 5)
suppressPackageStartupMessages(library(BinarySVMXHT))
library(BinarySVMXHT)
dataset = read.csv("StudentDepressionDataset.csv")
names(dataset)
head(dataset, 6)
plot_hist(dataset, "Gender")
plot_hist(dataset, "Age")
plot_hist(dataset, "City")
dataset = dataset[, -which(names(dataset) %in% c("id", "Job.Satisfaction", "Work.Pressure"))]
# Create a list which maps the character feature columns to numeric.
mapping = list(Gender = c("Female", "Male"),
Sleep.Duration = c("Less than 5 hours", "5-6 hours", "7-8 hours", "More than 8 hours", "Others"),
Dietary.Habits = c("Unhealthy", "Moderate", "Healthy"),
Have.you.ever.had.suicidal.thoughts.. = c("No", "Yes"),
Family.History.of.Mental.Illness = c("No", "Yes"))
dataset = preprocess(dataset, mapping, "Depression")
head(dataset, 6)
data = dataset[, 1:11]
label = dataset[, 12]
linear.model = svm(data[1:150, ], label[1:150])
linear.model
linear.model.partial = svm(data, label, n = 150, features = names(data)[1:8])
linear.model.partial
poly.model = svm(data, label, type = "linear", n = 300)
rbf.model = svm(data, label, type = "linear", n = 300)
random.test.index = sample(1:nrow(data), size = 200, replace = FALSE)
predict(linear.model, data[random.test.index, ])
test_accuracy(linear.model, data[random.test.index, ], label[random.test.index])
predict(linear.model.partial, data[random.test.index, ])
test_accuracy(linear.model.partial, data[random.test.index, ], label[random.test.index])
predict(poly.model.partial, data[random.test.index, ])
random.test.index = sample(1:nrow(data), size = 200, replace = FALSE)
predict(linear.model, data[random.test.index, ])
test_accuracy(linear.model, data[random.test.index, ], label[random.test.index])
predict(linear.model, data[random.test.index, ])
test_accuracy(linear.model.partial, data[random.test.index, ], label[random.test.index])
predict(poly.model, data[random.test.index, ])
test_accuracy(linear.model.partial, data[random.test.index, ], label[random.test.index])
predict(rbf.model, data[random.test.index, ])
test_accuracy(linear.model.partial, data[random.test.index, ], label[random.test.index])
random.test.index
poly.model$b
rbf.model$b
tune_parameter(data[1:100, ], label[1:100], data[101:200, ], label[101:200], type = "linear", C = c(1, 10, 100))
tune_parameter(data[1:200, ], label[1:200], data[201:400, ], label[201:400], type = "polynomial", C = c(1, 10, 100), degree = c(2, 3))
tune_parameter(data[1:200, ], label[1:200], data[201:400, ], label[201:400], type = "rbf", C = c(1, 10, 100), gamma = c(0.05, 0.1, 0.2))
tune_parameter(data[1:300, ], label[1:300], data[301:600, ], label[301:600], type = "linear", C = c(1, 10, 100))
tune_parameter(data[1:200, ], label[1:200], data[201:400, ], label[201:400], type = "polynomial", C = c(1, 10, 100), degree = c(2, 3))
tune_parameter(data[1:200, ], label[1:200], data[201:400, ], label[201:400], type = "rbf", C = c(1, 10, 100), gamma = c(0.05, 0.1, 0.2))
tune_parameter(data[1:100, ], label[1:100], data[101:200, ], label[101:200], type = "linear", C = c(1, 10, 100))
tune_parameter(data[1:200, ], label[1:200], data[201:400, ], label[201:400], type = "polynomial", C = c(1, 10, 100), degree = c(2, 3))
tune_parameter(data[1:200, ], label[1:200], data[201:400, ], label[201:400], type = "rbf", C = c(1, 10, 100), gamma = c(0.05, 0.1, 0.2))
kernel
knitr::opts_chunk$set(collapse = TRUE, comment = "#>", cache = FALSE,
autodep = FALSE, cache.comments = FALSE,
fig.width = 8, fig.height = 5)
suppressPackageStartupMessages(library(BinarySVMXHT))
library(BinarySVMXHT)
dataset = read.csv("StudentDepressionDataset.csv")
names(dataset)
head(dataset, 6)
plot_hist(dataset, "Gender")
plot_hist(dataset, "Age")
plot_hist(dataset, "City")
dataset = dataset[, -which(names(dataset) %in% c("id", "Job.Satisfaction", "Work.Pressure"))]
# Create a list which maps the character feature columns to numeric.
mapping = list(Gender = c("Female", "Male"),
Sleep.Duration = c("Less than 5 hours", "5-6 hours", "7-8 hours", "More than 8 hours", "Others"),
Dietary.Habits = c("Unhealthy", "Moderate", "Healthy"),
Have.you.ever.had.suicidal.thoughts.. = c("No", "Yes"),
Family.History.of.Mental.Illness = c("No", "Yes"))
dataset = preprocess(dataset, mapping, "Depression")
head(dataset, 6)
data = dataset[, 1:11]
label = dataset[, 12]
linear.model = svm(data[1:250, ], label[1:250])
linear.model
linear.model.partial = svm(data, label, n = 250, features = names(data)[1:8])
linear.model.partial
poly.model = svm(data, label, type = "polynomial", n = 250)
rbf.model = svm(data, label, type = "rbf", n = 250)
random.test.index = sample(1:nrow(data), size = 250, replace = FALSE)
predict(linear.model, data[random.test.index, ])
test_accuracy(linear.model, data[random.test.index, ], label[random.test.index])
predict(linear.model, data[random.test.index, ])
test_accuracy(linear.model.partial, data[random.test.index, ], label[random.test.index])
predict(poly.model, data[random.test.index, ])
test_accuracy(linear.model.partial, data[random.test.index, ], label[random.test.index])
predict(rbf.model, data[random.test.index, ])
test_accuracy(linear.model.partial, data[random.test.index, ], label[random.test.index])
select_features(data, label, n =  8, show = TRUE)
select_features(data, label, min = 0.2)
random.train.index = sample(1:nrow(data), size = 250, replace = FALSE)
tune_parameter(data[random.train.index, ], label[random.train.index],
data[random.test.index, ], label[random.test.index], type = "linear", C = c(1, 10, 100))
tune_parameter(data[random.train.index, ], label[random.train.index],
data[random.test.index, ], label[random.test.index], type = "polynomial", C = c(1, 10, 100), degree = c(2, 3))
tune_parameter(data[random.train.index, ], label[random.train.index],
data[random.test.index, ], label[random.test.index], type = "rbf", C = c(1, 10, 100), gamma = c(0.05, 0.1, 0.2))
knitr::opts_chunk$set(collapse = TRUE, comment = "#>", cache = FALSE,
autodep = FALSE, cache.comments = FALSE,
fig.width = 8, fig.height = 5)
suppressPackageStartupMessages(library(BinarySVMXHT))
library(BinarySVMXHT)
dataset = read.csv("StudentDepressionDataset.csv")
names(dataset)
head(dataset, 6)
plot_hist(dataset, "Gender")
plot_hist(dataset, "Age")
plot_hist(dataset, "City")
dataset = dataset[, -which(names(dataset) %in% c("id", "Job.Satisfaction", "Work.Pressure"))]
# Create a list which maps the character feature columns to numeric.
mapping = list(Gender = c("Female", "Male"),
Sleep.Duration = c("Less than 5 hours", "5-6 hours", "7-8 hours", "More than 8 hours", "Others"),
Dietary.Habits = c("Unhealthy", "Moderate", "Healthy"),
Have.you.ever.had.suicidal.thoughts.. = c("No", "Yes"),
Family.History.of.Mental.Illness = c("No", "Yes"))
dataset = preprocess(dataset, mapping, "Depression")
head(dataset, 6)
data = dataset[, 1:11]
label = dataset[, 12]
linear.model = svm(data[1:250, ], label[1:250])
linear.model
linear.model.partial = svm(data, label, n = 250, features = names(data)[1:8])
linear.model.partial
poly.model = svm(data, label, type = "polynomial", n = 250)
rbf.model = svm(data, label, type = "rbf", n = 250)
random.test.index = sample(1:nrow(data), size = 250, replace = FALSE)
predict(linear.model, data[random.test.index, ])
test_accuracy(linear.model, data[random.test.index, ], label[random.test.index])
predict(linear.model, data[random.test.index, ])
test_accuracy(linear.model.partial, data[random.test.index, ], label[random.test.index])
predict(poly.model, data[random.test.index, ])
test_accuracy(linear.model.partial, data[random.test.index, ], label[random.test.index])
predict(rbf.model, data[random.test.index, ])
test_accuracy(linear.model.partial, data[random.test.index, ], label[random.test.index])
select_features(data, label, n =  8, show = TRUE)
select_features(data, label, min = 0.2)
random.train.index = sample(1:nrow(data), size = 250, replace = FALSE)
tune_parameter(data[random.train.index, ], label[random.train.index],
data[random.test.index, ], label[random.test.index], type = "linear", C = c(1, 10, 100))
tune_parameter(data[random.train.index, ], label[random.train.index],
data[random.test.index, ], label[random.test.index], type = "polynomial", C = c(1, 10, 100), degree = c(2, 3))
tune_parameter(data[random.train.index, ], label[random.train.index],
data[random.test.index, ], label[random.test.index], type = "rbf", C = c(1, 10, 100), gamma = c(0.05, 0.1, 0.2))
random.test.index.2 = sample(1:nrow(data), size = 250, replace = FALSE)
linear.model.tuned = svm(data, label, type = "linear", n = 250, C = 10)
test_accuracy(linear.model, data[random.test.index.2, ], label[random.test.index.2])
poly.model.tuned = svm(data, label, type = "polynomial", n = 250, degree = 3)
test_accuracy(linear.model, data[random.test.index.2, ], label[random.test.index.2])
rbf.model.tuned = svm(data, label, type = "rbf", n = 250, C = 1, gamma = 0.05)
test_accuracy(linear.model, data[random.test.index.2, ], label[random.test.index.2])
knitr::opts_chunk$set(collapse = TRUE, comment = "#>", cache = FALSE,
autodep = FALSE, cache.comments = FALSE,
fig.width = 8, fig.height = 5)
suppressPackageStartupMessages(library(BinarySVMXHT))
library(BinarySVMXHT)
dataset = read.csv("StudentDepressionDataset.csv")
names(dataset)
head(dataset, 6)
plot_hist(dataset, "Gender")
plot_hist(dataset, "Age")
plot_hist(dataset, "City")
dataset = dataset[, -which(names(dataset) %in% c("id", "Job.Satisfaction", "Work.Pressure"))]
# Create a list which maps the character feature columns to numeric.
mapping = list(Gender = c("Female", "Male"),
Sleep.Duration = c("Less than 5 hours", "5-6 hours", "7-8 hours", "More than 8 hours", "Others"),
Dietary.Habits = c("Unhealthy", "Moderate", "Healthy"),
Have.you.ever.had.suicidal.thoughts.. = c("No", "Yes"),
Family.History.of.Mental.Illness = c("No", "Yes"))
dataset = preprocess(dataset, mapping, "Depression")
head(dataset, 6)
data = dataset[, 1:11]
label = dataset[, 12]
linear.model = svm(data[1:250, ], label[1:250])
linear.model
linear.model.partial = svm(data, label, n = 250, features = names(data)[1:8])
linear.model.partial
poly.model = svm(data, label, type = "polynomial", n = 250)
rbf.model = svm(data, label, type = "rbf", n = 250)
random.test.index = sample(1:nrow(data), size = 250, replace = FALSE)
predict(linear.model, data[random.test.index, ])
test_accuracy(linear.model, data[random.test.index, ], label[random.test.index])
predict(linear.model, data[random.test.index, ])
test_accuracy(linear.model.partial, data[random.test.index, ], label[random.test.index])
predict(poly.model, data[random.test.index, ])
test_accuracy(poly.model, data[random.test.index, ], label[random.test.index])
predict(rbf.model, data[random.test.index, ])
test_accuracy(rbf.model, data[random.test.index, ], label[random.test.index])
select_features(data, label, n =  8, show = TRUE)
select_features(data, label, min = 0.2)
random.train.index = sample(1:nrow(data), size = 250, replace = FALSE)
tune_parameter(data[random.train.index, ], label[random.train.index],
data[random.test.index, ], label[random.test.index], type = "linear", C = c(1, 10, 100))
tune_parameter(data[random.train.index, ], label[random.train.index],
data[random.test.index, ], label[random.test.index], type = "polynomial", C = c(1, 10, 100), degree = c(2, 3))
tune_parameter(data[random.train.index, ], label[random.train.index],
data[random.test.index, ], label[random.test.index], type = "rbf", C = c(1, 10, 100), gamma = c(0.05, 0.1, 0.2))
random.test.index.2 = sample(1:nrow(data), size = 250, replace = FALSE)
linear.model.tuned = svm(data, label, type = "linear", n = 250, C = 10)
test_accuracy(linear.model.tuned, data[random.test.index.2, ], label[random.test.index.2])
poly.model.tuned = svm(data, label, type = "polynomial", n = 250, degree = 3)
test_accuracy(poly.model.tuned, data[random.test.index.2, ], label[random.test.index.2])
rbf.model.tuned = svm(data, label, type = "rbf", n = 250, C = 1, gamma = 0.05)
test_accuracy(rbf.model.tuned, linear.model, data[random.test.index.2, ], label[random.test.index.2])
random.test.index.2 = sample(1:nrow(data), size = 250, replace = FALSE)
linear.model.tuned = svm(data, label, type = "linear", n = 250, C = 10)
test_accuracy(linear.model.tuned, data[random.test.index.2, ], label[random.test.index.2])
poly.model.tuned = svm(data, label, type = "polynomial", n = 250, degree = 3)
test_accuracy(poly.model.tuned, data[random.test.index.2, ], label[random.test.index.2])
rbf.model.tuned = svm(data, label, type = "rbf", n = 250, C = 1, gamma = 0.05)
test_accuracy(rbf.model.tuned, data[random.test.index.2, ], label[random.test.index.2])
random.train.index
random.train.index = sample(1:nrow(data), size = 250, replace = FALSE)
tune_parameter(data[random.train.index, ], label[random.train.index],
data[random.test.index, ], label[random.test.index], type = "linear", C = c(1, 10, 100))
tune_parameter(data[random.train.index, ], label[random.train.index],
data[random.test.index, ], label[random.test.index], type = "polynomial", C = c(1, 10, 100), degree = c(2, 3))
tune_parameter(data[random.train.index, ], label[random.train.index],
data[random.test.index, ], label[random.test.index], type = "rbf", C = c(1, 10, 100), gamma = c(0.05, 0.1, 0.2))
random.test.index.2 = sample(1:nrow(data), size = 250, replace = FALSE)
linear.model.tuned = svm(data, label, type = "linear", n = 250, C = 10)
test_accuracy(linear.model.tuned, data[random.test.index.2, ], label[random.test.index.2])
poly.model.tuned = svm(data, label, type = "polynomial", n = 250, degree = 3)
test_accuracy(poly.model.tuned, data[random.test.index.2, ], label[random.test.index.2])
rbf.model.tuned = svm(data, label, type = "rbf", n = 250, C = 1, gamma = 0.2)
test_accuracy(rbf.model.tuned, data[random.test.index.2, ], label[random.test.index.2])
devtools::build_vignettes()
par(mfrow = c(2, 2), mar = c(4, 4, 2, 0.5), mgp = c(2.5, 1, 0))
plot_hist(dataset, "Gender")
plot_hist(dataset, "Age")
plot_hist(dataset, "City")
library(BinarySVMXHT)
dataset = read.csv("StudentDepressionDataset.csv")
names(dataset)
head(dataset, 6)
grid.arrange(plot_hist(dataset, "Gender"), plot_hist(dataset, "Age"), plot_hist(dataset, "City"), plot_hist(dataset, "Profession"), ncol = 2)
library(ggplot2)
grid.arrange(plot_hist(dataset, "Gender"), plot_hist(dataset, "Age"), plot_hist(dataset, "City"), plot_hist(dataset, "Profession"), ncol = 2)
install.packages(gridExtra)
install.packages("gridExtra")
library(gridExtra)
grid.arrange(plot_hist(dataset, "Gender"), plot_hist(dataset, "Age"), plot_hist(dataset, "City"), plot_hist(dataset, "Profession"), ncol = 2)
plot_hist(dataset, "Profession")
plot_hist(dataset, "Gender")
plot_hist(dataset, "Age")
plot_hist(dataset, "City")
plot_hist(dataset, "Profession")
plot_hist(dataset, "Profession")
devtools::build_vignettes()
num.fac <- factor(c(3.4, 1.2, 5))
num.fac
as.character(num.fac)
as.numeric(num.fac)
num.fac[as.numeric(num.fac)]
levels(num.fac)[num.fac]
levels(num.fac)
num.fac <- factor(c(3.4, 1.2, 5))
as.numeric(levels(num.fac)[num.fac])
class(state.x77)
state.df = data.frame(state.x77, Region=state.region, Division=state.division)
class(state.df)
tapply(state.x77[,"Frost"], INDEX=state.region, FUN=mean)
head(state.x77)
state.region
mtcars
summarize(group_by(mtcars, cyl), mpg = mean(mpg), hp = mean(hp))
library(purrr)
summarize(group_by(mtcars, cyl), mpg = mean(mpg), hp = mean(hp))
library(dplyr)
summarize(group_by(mtcars, cyl), mpg = mean(mpg), hp = mean(hp))
tapply(mtcars, INDEX=mtcars$cyl, FUN= function(x) {
return(c(mean(x$mpg), mean(x$hp)))
})
tapply(mtcars, INDEX=mtcars$cyl, FUN= function(x) {
return(mean(x$mpg))})
list(mtcars$cyl)
v
sapply(split(mtcars, mtcars$cyl), FUN=function(df) {
return(c("mpg" = mean(df$mpg), "hp" = mean(df$hp)))
})
split(mtcars, mtcars$cyl)
typeof(sapply(split(mtcars, mtcars$cyl), FUN=function(df) {
return(c("mpg" = mean(df$mpg), "hp" = mean(df$hp)))
}))
class(sapply(split(mtcars, mtcars$cyl), FUN=function(df) {
return(c("mpg" = mean(df$mpg), "hp" = mean(df$hp)))
}))
class(state.x77)
iris %>%
group_by(Species) %>%
summarize(Sepal.Length.Mean = mean(Sepal.Length))
iris %>%
group_by(Species) %>%
summarize(Sepal.Length.Mean = mean(Sepal.Length)) %>%
ggplot(aes(x = Species, y = Sepal.Length.Mean)) +
iris %>%
group_by(Species) %>%
summarize(Sepal.Length.Mean = mean(Sepal.Length)) %>%
ggplot(aes(x = Species, y = Sepal.Length.Mean))
library(ggplot2)
iris %>%
group_by(Species) %>%
summarize(Sepal.Length.Mean = mean(Sepal.Length)) %>%
ggplot(aes(x = Species, y = Sepal.Length.Mean))
" (", 42:38, ")"
(", 42:38, ")
[:alpha:]
"[:alpha:]"
"\\n"
surface = function(expr, from.x=0, to.x=1, from.y=0, to.y=1, n.x=30,
n.y=30, col.list=rainbow(30), theta=5, phi=25, mar=c(1,1,1,1), ...) {
# Build the 2d grid
x = seq(from=from.x,to=to.x,length.out=n.x)
y = seq(from=from.y,to=to.y,length.out=n.y)
plot.grid = expand.grid(x=x,y=y)
# Evaluate the expression to get matrix of z values
uneval.expr = substitute(expr)
z.vals = eval(uneval.expr,envir=plot.grid)
z = matrix(z.vals,nrow=n.x)
# Figure out margins
orig.mar = par()$mar # Save the original margins
par(mar=mar)
col.grid = col.list[round((z[-1,-1]-min(z))/(max(z)-min(z))
* (length(col.list)-1) + 1)]
r = persp(x,y,z,theta=theta,phi=phi)
par(mar=orig.mar) # Restore the original margins
invisible(r) # Return the persp object invisibly
}
surface(x^3 + y^3, from.x=-3, to.x=3, from.y=-3, to.y=3)
1:3
1:c
add.up.inv.powers = function(n, verbose=FALSE) {
x = 0
# browser()
for (i in 1:n) {
x = x + i^(1/i)
if (verbose) roman.cat(i)
}
if (verbose) cat("\n")
return(x)
}
roman.cat = function(num) {
roman.num = as.roman(num)
roman.str = as.character(roman.num)
cat(roman.str, "... ")
}
test_that("add.up.inv.powers() fails for non-integer n", {
expect_error(add.up.inv.powers(n="c", verb=FALSE))
})
library(testthat)
library(testthat)
test_that("add.up.inv.powers() fails for non-integer n", {
expect_error(add.up.inv.powers(n="c", verb=FALSE))
})
test_that("add.up.inv.powers() fails for non-integer n", {
expect_warning(add.up.inv.powers(n="c", verb=FALSE))
})
expect_error(add.up.inv.powers(n="c", verb=FALSE))
add.up.inv.powers(n="c", verb=FALSE)
1:c
1:c
got_chars
library(dplyr)
got_chars
got_chars
library(purrr)
got_chars
library(repurrrsive)
got_chars
got_chars[["names"]]
got_chars[["name"]]
sapply(got_chars, `[[`, "name")
sapply(got_chars, `[[`, "gender")
sapply(got_chars, `[[`, "gender") {}
substr(sapply(got_chars, `[[`, "gender"), 1, 1)
sapply(got_chars, function(x) {})
sapply(got_chars, function(x) {})
sapply(got_chars, function(x) {
if (x[["gender"] == "Male"]) {}
if (x[["gender"] == "Male"]) {}
sapply(got_chars, function(x) {
if (x[["gender"]] == "Male") {
"M"
}
else {
"F"
}
})
seq(1, 12, by. = 3)
seq(1, 12, by = 3)
seq(1, 13, by = 3)
seq(1, 16, length . out = 6)
seq(1, 16, length.out = 6)
seq(1, 16, length.out = 5)
seq(1, 16, length.out = 6)
grep("^F.*F$", c("F","FG","GF","FF","FaFa","FabcF"), value = TRUE)
grep("^F.*F$", c("F","FG","GF","FF","FaFa","FabcF"), value = FALSE)
emails
library(dplyr)
emails
c(1, 2)
m = c(1, 2)
m[1/5]
m[1.5]
m[2/1.5]
2:2
fibonacci = function ( n ) {
if ( n ==1 | n ==2){
fib = 1
return ( fib )}
else {
for ( i in 2:( n -1)) {
fib [ i +1] = fib [ i ] + fib [i -1]}
return ( fib [ n ])
}
}
fibonacci(4)
fib = c(1, 2)
fibonacci(4)
fib = c(1, 1)
fibonacci(4)
dbListTables(con)
library(DBI)
library(RSQLite)
drv = dbDriver("SQLite")
con = dbConnect(drv, dbname = "lahman2016.sqlite")
dbListTables(con)
getwdd
getwd()
setwd("/Users/Leviathan/Downloads/STA3005/Assignment 6")
library(DBI)
library(RSQLite)
drv = dbDriver("SQLite")
con = dbConnect(drv, dbname = "lahman2016.sqlite")
dbListTables(con)
colnames(batting)
batting = dbReadTable(con, "Batting")
colnames(batting)
dbListFields(con, "Salaries")
dbGetQuery(con, paste("SELECT *,
"FROM Batting JOIN Salaries USING(yearID, playerID)",
dbGetQuery(con, paste("SELECT *",
"FROM Batting JOIN Salaries USING(yearID, playerID)",
"LIMIT 10"))
dbGetQuery(con, paste("SELECT *",
"FROM Batting LEFT JOIN Salaries USING(yearID, playerID)",
"LIMIT 10"))
dbGetQuery(con, paste("SELECT *",
"FROM Batting FULL JOIN Salaries USING(yearID, playerID)",
"LIMIT 10"))
dbGetQuery(con, paste("SELECT *",
"FROM Batting RIGHT JOIN Salaries USING(yearID, playerID)",
"LIMIT 10"))
dbGetQuery(con, paste("SELECT *",
"FROM Salaries Left JOIN Batting USING(yearID, playerID)",
"LIMIT 10"))
dbGetQuery(con, paste("SELECT *",
"FROM Batting RIGHT JOIN Salaries USING(yearID, playerID)",
"LIMIT 10"))
log(2)
